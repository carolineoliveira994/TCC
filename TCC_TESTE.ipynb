{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMqbTR5+4PSCOs7vVuRDi+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolineoliveira994/100-days-of-code-python/blob/main/TCC_TESTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KIU4C5hNWtRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Depejndencias Bibliotecas italicized text"
      ],
      "metadata": {
        "id": "BfXEb4ImWwTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install python-dotenv\n",
        "!pip install spacy textblob\n",
        "!python -m textblob.download_corpora\n",
        "!python -m spacy download pt_core_news_sm  # Modelo em português\n",
        "!pip install unidecode\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "!pip install unidecode nltk spacy\n",
        "!python -m spacy download pt_core_news_sm\n"
      ],
      "metadata": {
        "id": "iIRcujxg6Ttu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from dotenv import load_dotenv\n",
        "import spacy\n",
        "import re\n",
        "import unidecode\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import SnowballStemmer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "\n",
        "from google.colab import files, auth\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "print(\"Diretório atual:\", os.getcwd())"
      ],
      "metadata": {
        "id": "jJUnZI496N5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WvkMFiP3vAsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coleta de dados usando a API da Bluesky\n",
        "\n",
        "\n",
        "**Objetivo**: Obter dados relevantes de postagens, comentários e interações na rede social Bluesky, focando em sentimentos relacionados às eleições de 2024.\n",
        "\n",
        "**Fontes de Dados**: Bluesky.\n",
        "\n",
        "**Técnicas de Coleta:** API do ATProtocol para interagir com os servidores Bluesky.\n",
        "\n",
        "**Autenticação**: Realizar a autenticação necessária para acessar a API, salvar credenciais no arquito .env.\n",
        "\n",
        "**Requisições HTTP:** Endpoints específicos da API que permitem obter postagens, comentários e perfis de usuários.\n",
        "\n",
        "PALAVRAS CHAVES: ELEIÇÕES, CANDITADOS, BOULOS, TABATA, DATENA...\n",
        "\n",
        "**Período de Coleta**: JUNHO.\n"
      ],
      "metadata": {
        "id": "5MlaxnCfs3Ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nrkdzu6FrdZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Packages:"
      ],
      "metadata": {
        "id": "sWiE_VsXvJp1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrTiJrTIbOKJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files, auth\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JX8HyCgDvolW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('.env', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "BLUESKY_APP_USER = 'cocorolini.bsky.social'\n",
        "BLUESKY_APP_PASS='3ovj-gbnh-trwd-k66r'\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "zs6iabB95QHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Busca no BLUESKY com a palavra chave \"ELEIÇÕES\""
      ],
      "metadata": {
        "id": "LJkfVJ5rwFGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py search partido --sort latest --limit 90\n"
      ],
      "metadata": {
        "id": "1pHULWJI5TTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "file_path = '/content/data/search_results_BOULOS_2024_09_29.csv'\n",
        "df_BOULOS = pd.read_csv(file_path)\n",
        "df_BOULOS"
      ],
      "metadata": {
        "id": "lCZ-VRwhRBhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_Boulos_2024_09_29.csv'\n",
        "df_Boulos = pd.read_csv(file_path)\n",
        "df_Boulos"
      ],
      "metadata": {
        "id": "S7zDaTFnRBLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_CAMPANHA_2024_09_29.csv'\n",
        "df_CAMPANHA = pd.read_csv(file_path)\n",
        "df_CAMPANHA"
      ],
      "metadata": {
        "id": "Ab90C5ACJtc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_CANDIDATO_2024_09_29.csv'\n",
        "df_CANDIDATO = pd.read_csv(file_path)\n",
        "df_CANDIDATO"
      ],
      "metadata": {
        "id": "cm4Q1_zsJtUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_DATENA_2024_09_29.csv'\n",
        "df_DATENA = pd.read_csv(file_path)\n",
        "df_DATENA"
      ],
      "metadata": {
        "id": "56hURl_wJtLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_Datena_2024_09_29 (1).csv'\n",
        "df_Datena = pd.read_csv(file_path)\n",
        "df_Datena"
      ],
      "metadata": {
        "id": "3KZaj0zgJtCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_ELEIÇÕES_2024_09_29.csv'\n",
        "df_ELEIÇÕES = pd.read_csv(file_path)\n",
        "df_ELEIÇÕES"
      ],
      "metadata": {
        "id": "qMyG-NvFJs40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_MARÇAL_2024_09_29.csv'\n",
        "df_MARÇAL = pd.read_csv(file_path)\n",
        "df_MARÇAL"
      ],
      "metadata": {
        "id": "CAXByQD5Jsv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_CAMPANHA_2024_09_29.csv'\n",
        "df_CAMPANHA = pd.read_csv(file_path)\n",
        "df_CAMPANHA"
      ],
      "metadata": {
        "id": "394hA17YJsoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_NUNES_2024_09_29.csv'\n",
        "df_NUNES = pd.read_csv(file_path)\n",
        "df_NUNES"
      ],
      "metadata": {
        "id": "0RzErCmDJsfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_Nunes_2024_09_29.csv'\n",
        "df_Nunes = pd.read_csv(file_path)\n",
        "df_Nunes"
      ],
      "metadata": {
        "id": "eDZhChqFJsW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_TABATA_2024_09_29.csv'\n",
        "df_TABATA = pd.read_csv(file_path)\n",
        "df_TABATA"
      ],
      "metadata": {
        "id": "Dsv_Q0pCJsM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_Tabata_2024_09_29.csv'\n",
        "df_Tabata = pd.read_csv(file_path)\n",
        "df_Tabata"
      ],
      "metadata": {
        "id": "vYDpGM-WJsDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_campanha_2024_09_29.csv'\n",
        "df_campanha = pd.read_csv(file_path)\n",
        "df_campanha"
      ],
      "metadata": {
        "id": "Uj9PXMnBJr6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_candidato_2024_09_29.csv'\n",
        "df_candidato = pd.read_csv(file_path)\n",
        "df_candidato"
      ],
      "metadata": {
        "id": "xQyKOf5JJrwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_datena_2024_09_29.csv'\n",
        "df_datena = pd.read_csv(file_path)\n",
        "df_datena"
      ],
      "metadata": {
        "id": "bXHXdw06Jrlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_marçal_2024_09_29 (2).csv'\n",
        "df_CAMPANHA = pd.read_csv(file_path)\n",
        "df_CAMPANHA"
      ],
      "metadata": {
        "id": "jN78vkJQJ6mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_eleicoes_2024_09_29.csv'\n",
        "df_eleicoes = pd.read_csv(file_path)\n",
        "df_eleicoes"
      ],
      "metadata": {
        "id": "Zk0F5kJeJ7EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_nunes_2024_09_29 (1).csv'\n",
        "df_nunes = pd.read_csv(file_path)\n",
        "df_nunes"
      ],
      "metadata": {
        "id": "95HBvUr-J8EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_tabata_2024_09_29.csv'\n",
        "df_tabata = pd.read_csv(file_path)\n",
        "df_tabata"
      ],
      "metadata": {
        "id": "HV2k_vOdJ727"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_CAMPANHA_2024_09_29.csv'\n",
        "df_CAMPANHA = pd.read_csv(file_path)\n",
        "df_CAMPANHA"
      ],
      "metadata": {
        "id": "b_6E4CDSJ7cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_Política_2024_09_29.csv'\n",
        "df_Política = pd.read_csv(file_path)\n",
        "df_Política"
      ],
      "metadata": {
        "id": "gwwBpBTHJOz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_VOTO_2024_09_29.csv'\n",
        "df_VOTO = pd.read_csv(file_path)\n",
        "df_VOTO"
      ],
      "metadata": {
        "id": "2WRo0JLVJR5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_entrevista_2024_09_29.csv'\n",
        "df_entrevistas = pd.read_csv(file_path)\n",
        "df_entrevistas"
      ],
      "metadata": {
        "id": "wWoVOIlrK59E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_partido_2024_09_29.csv'\n",
        "df_partido = pd.read_csv(file_path)\n",
        "df_partido"
      ],
      "metadata": {
        "id": "dF9Skzp8K53W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_urna_2024_09_29.csv'\n",
        "df_urna = pd.read_csv(file_path)\n",
        "df_urna"
      ],
      "metadata": {
        "id": "961MArquK5og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/search_results_voto_2024_09_29.csv'\n",
        "df_voto = pd.read_csv(file_path)\n",
        "df_voto"
      ],
      "metadata": {
        "id": "7LxMIuxZK5l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cria** DataFrame"
      ],
      "metadata": {
        "id": "J3vrlGg-wIEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lista DataFrame criados"
      ],
      "metadata": {
        "id": "aJTVuhxAwMsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_names = [name for name, obj in globals().items() if isinstance(obj, pd.DataFrame)]\n",
        "print(dataframe_names)"
      ],
      "metadata": {
        "id": "XIwurn5p66NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Junta os DataFrames\n",
        "\n"
      ],
      "metadata": {
        "id": "XGhfGpuSwRYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de nomes de DataFrames corrigida (sem aspas únicas extras)\n",
        "dataframes = ['df_CAMPANHA', 'df_CANDIDATO', 'df_DATENA', 'df_Datena', 'df_ELEIÇÕES', 'df_MARÇAL', 'df_NUNES', 'df_Nunes', 'df_TABATA', '_39', 'df_Tabata', '_40', 'df_campanha', '_41', 'df_candidato', '_42', 'df_datena', '_44', '_45', 'df_eleicoes', '_46', 'df_marçal', '_47', 'df_nunes', '_48', 'df_tabata', 'df_concatenado', 'df_boulos', 'df_BOULOS', 'df_Boulos', 'df_Política', 'df_VOTO', 'df_entrevistas', 'df_urna',  'df_voto']\n",
        "\n",
        "# Concatenar apenas os DataFrames que existem no escopo\n",
        "dataframes_to_concat = [globals()[name] for name in dataframes if name in globals()]\n",
        "\n",
        "# Concatenar os DataFrames em um único DataFrame\n",
        "df_concatenado = pd.concat(dataframes_to_concat, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame concatenado\n",
        "print(df_concatenado)\n"
      ],
      "metadata": {
        "id": "gN6qTCjYHuCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_W3UT2WGIwie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_concatenado"
      ],
      "metadata": {
        "id": "ukIr_XUFHkMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remover colunas não ultilizadas"
      ],
      "metadata": {
        "id": "N3dnOmzkwfcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = [\"reply_count\", \"author_display_name\", \"author_handle\", \"indexed_at\", \"cid\", \"uri\", \"repost_count\", \"like_count\"]\n",
        "df_concatenado = df_concatenado.drop(columns=columns_to_remove)\n",
        "\n",
        "print(\"\\nDepois:\")\n",
        "print(df_concatenado)\n"
      ],
      "metadata": {
        "id": "cKAa0Sv9JgES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limpeza"
      ],
      "metadata": {
        "id": "vc6D1jhMJ4EM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpeza: Remoção de stop words: Palavras comuns que não adicionam significado (ex: \"a\", \"o\", \"de\").\n",
        "Remoção de pontuação: Pontuação pode interferir na análise.\n",
        "Remoção de números: Se não forem relevantes para a análise de sentimentos.\n",
        "Correção de erros de digitação: Utilizando técnicas de correção ortográfica."
      ],
      "metadata": {
        "id": "-1_kWvYvKgtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autenticação hugg****ntirar"
      ],
      "metadata": {
        "id": "AVIYDADkwk10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "os.environ['HF_TOKEN'] = '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'\n"
      ],
      "metadata": {
        "id": "pZ-eMcvDP2bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparar o BERT para análise de sentimentos com três rótulos (positivo, negativo e neutro)"
      ],
      "metadata": {
        "id": "imnbIZ1-wunc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=3)\n"
      ],
      "metadata": {
        "id": "n0W6A385NkOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fluxo de Pré-processamento com spaCy\n",
        "\n",
        "* Remoção de URLs.\n",
        "* Remoção de menções.\n",
        "* Remoção de hashtags.\n",
        "* Remoção de números.\n",
        "* Remoção de acentos.\n",
        "* Conversão para minúsculas\n",
        "* Lematização e remoção de stop words e pontuação: Usa o spaCy para remover stop words e pontuação e retorna o texto lematizado.\n"
      ],
      "metadata": {
        "id": "1hyGhaLFxfGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Retorna uma string vazia se o texto não for string\n",
        "\n",
        "    # Remover URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remover menções (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remover hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Remover números\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remover acentos\n",
        "    text = unidecode.unidecode(text)\n",
        "\n",
        "    # Converter para minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Processar o texto com spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Remover stop words e pontuação\n",
        "    cleaned_text = ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "texts = df_concatenado['text'].tolist()\n",
        "cleaned_texts = [clean_text(text) for text in texts]\n",
        "\n",
        "df_concatenado['cleaned_text'] = cleaned_texts\n",
        "print(df_concatenado[['text', 'cleaned_text']].head())\n"
      ],
      "metadata": {
        "id": "pi06AHArKjVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicar a função para rotular automaticamente os textos"
      ],
      "metadata": {
        "id": "SwLKDv59yHBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribuição de Comprimento de Textos"
      ],
      "metadata": {
        "id": "jLrychjJID1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição das listas de palavras-chave (Features)"
      ],
      "metadata": {
        "id": "-pgEL10CyuyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Carregar a coluna 'cleaned_text' do DataFrame\n",
        "data = df_concatenado['cleaned_text']\n",
        "df_concatenado = pd.DataFrame(data)\n",
        "\n",
        "# Carregar o pipeline de análise de sentimentos com modelo pré-treinado\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "# Mapeamento dos rótulos para categorias\n",
        "label_mapping = {\n",
        "    '1 star': 'negativo',\n",
        "    '2 stars': 'negativo',\n",
        "    '3 stars': 'neutro',\n",
        "    '4 stars': 'positivo',\n",
        "    '5 stars': 'positivo'\n",
        "}\n",
        "\n",
        "# Classificar sentimentos e mapear para os rótulos desejados\n",
        "df_concatenado['sentiment'] = df_concatenado['cleaned_text'].apply(lambda x: label_mapping[sentiment_pipeline(x)[0]['label']])\n",
        "\n",
        "# Visualizar o DataFrame com os rótulos\n",
        "print(df_concatenado)\n"
      ],
      "metadata": {
        "id": "GBemsIl6Hh-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_concatenado.shape"
      ],
      "metadata": {
        "id": "jlK4qJfbQlnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_concatenado"
      ],
      "metadata": {
        "id": "8NBvkvPtL-5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_test_split da biblioteca sklearn é utilizada para dividir os dados em duas partes, uma para treinamento do modelo e outra para avaliação teste\n",
        "\n",
        "mportância da Divisão Estratificada:\n",
        "Como você está lidando com um problema de análise de sentimentos com três rótulos (positivo, negativo, neutro), é importante garantir que o modelo veja uma proporção representativa de cada classe durante o treinamento e também durante a avaliação. Isso é especialmente relevante quando as classes são desbalanceadas, como no seu caso (com muito mais instâncias positivas do que negativas e neutras).\n",
        "\n",
        "Exemplo Prático:\n",
        "Se você tem um total de 2641 instâncias no DataFrame df_concatenado, a divisão estratificada irá garantir algo como:\n",
        "\n",
        "Treinamento: Aproximadamente 2112 instâncias (80% do total), distribuídas de maneira proporcional aos rótulos.\n",
        "Avaliação: Aproximadamente 529 instâncias (20% do total), também com a mesma distribuição proporcional."
      ],
      "metadata": {
        "id": "LMwOkQXxzsb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, eval_df = train_test_split(df_concatenado, test_size=0.2, random_state=42, stratify=df_concatenado['sentiment'])\n",
        "\n",
        "print(\"Dados de Treinamento:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nDados de Avaliação:\")\n",
        "print(eval_df.head())\n"
      ],
      "metadata": {
        "id": "JtrshX7zaZ9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar a contagem de exemplos por classe\n",
        "print(df_concatenado['sentiment'].value_counts())\n"
      ],
      "metadata": {
        "id": "VyGsbyUHDkR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_concatenado['sentiment'].value_counts())\n"
      ],
      "metadata": {
        "id": "6iRTqTN7a2yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizar os textos usando o BERT em português\n",
        "\n",
        "**input_ids**: IDs numéricos correspondentes aos tokens do texto.\n",
        "\n",
        "**attention_mask**: Máscara de atenção que indica quais tokens são reais e quais são padding (zeros)."
      ],
      "metadata": {
        "id": "cudGJiMt01ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Carregar o tokenizer do modelo pré-treinado em português\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# Função para tokenizar os textos\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Tokenizar os dados de treinamento e de avaliação\n",
        "train_encodings = tokenize_function(train_df['cleaned_text'].tolist())\n",
        "eval_encodings = tokenize_function(eval_df['cleaned_text'].tolist())\n",
        "\n",
        "# Exibir a estrutura dos encodings gerados\n",
        "print(train_encodings.keys())  # Mostra as chaves, como 'input_ids', 'attention_mask'\n",
        "print(eval_encodings.keys())\n"
      ],
      "metadata": {
        "id": "a4TCF2JpgX_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "teste"
      ],
      "metadata": {
        "id": "D8JkT0jVViyU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftYfksDSncLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definir o SentimentDataset e os loaders de dados de treinamento e avaliação.\n",
        "\n",
        "**SentimentDataset**: organiza os dados de entrada (codificações e rótulos) para que possam ser usados pelo modelo.\n",
        "\n",
        "**DataLoader**: facilita o carregamento dos dados em lotes para alimentar o modelo durante o treinamento e a avaliação."
      ],
      "metadata": {
        "id": "Q6Oa8hOR1W9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SentimentAnalysisDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Usar .clone().detach() para evitar o aviso\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "\n",
        "        # Garantir que os rótulos estejam no formato correto\n",
        "        item['sentiment'] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Mapear os rótulos para números usando o mapeamento\n",
        "label_mapping = {\"positivo\": 2, \"neutro\": 1, \"negativo\": 0}\n",
        "train_labels = [label_mapping[label] for label in train_df['sentiment']]\n",
        "eval_labels = [label_mapping[label] for label in eval_df['sentiment']]\n",
        "\n",
        "# Criar os datasets para treinamento e avaliação\n",
        "train_dataset = SentimentAnalysisDataset(train_encodings, train_labels)\n",
        "eval_dataset = SentimentAnalysisDataset(eval_encodings, eval_labels)\n"
      ],
      "metadata": {
        "id": "seVXyhnegaz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Carregar o modelo BERT para classificação com 3 rótulos (positivo, neutro, negativo)\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=3)\n"
      ],
      "metadata": {
        "id": "L2no0EGWIYhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJvv1AAhffXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Definir o otimizador (usando Adam, mas pode ser alterado para outro otimizador)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)  # lr = taxa de aprendizado (learning rate)\n",
        "\n",
        "# Definir a função de perda (loss function)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()  # Como você tem rótulos categóricos (positivo, neutro, negativo), CrossEntropyLoss é adequada"
      ],
      "metadata": {
        "id": "XcayHb1me__p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "curva ROC e a área sob a curva (AUC) poderiam ser utilizadas para avaliar o desempenho do modelo, especialmente em casos de desbalanceamento de classes."
      ],
      "metadata": {
        "id": "dIZ4HIjbnIr-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qISiT8-kkhjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AdamW\n",
        "\n",
        "# Defina o número de épocas\n",
        "num_epochs = 5  # Defina o número de épocas que você deseja treinar\n",
        "\n",
        "# Defina o otimizador com L2 regularization (weight_decay)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Adiciona L2 regularization com weight_decay=0.01\n",
        "\n",
        "# Inicialize listas para armazenar as métricas\n",
        "train_precisions = []\n",
        "train_recalls = []\n",
        "train_f1s = []\n",
        "train_accuracies = []\n",
        "\n",
        "# Loop de treinamento\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Treinamento\n",
        "    model.train()  # Certifique-se de que o modelo está no modo de treino (Dropout ativado)\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "    train_loss_total = 0  # Inicializar o acumulador de perda\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "        loss = loss_fn(outputs.logits, batch['sentiment'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Somar a perda\n",
        "        train_loss_total += loss.item()\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        train_preds.extend(preds)\n",
        "        train_labels.extend(batch['sentiment'].cpu().numpy())\n",
        "\n",
        "    # Calcular métricas no treino\n",
        "    train_accuracy = accuracy_score(train_labels, train_preds)\n",
        "    train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(train_labels, train_preds, average='weighted')\n",
        "    train_precisions.append(train_precision)\n",
        "    train_recalls.append(train_recall)\n",
        "    train_f1s.append(train_f1)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Exibir resultados do treino\n",
        "    print(f\"Train Loss: {train_loss_total / len(train_loader):.4f}\")\n",
        "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\")\n",
        "\n",
        "# Calcular a matriz de confusão\n",
        "conf_matrix = confusion_matrix(train_labels, train_preds)\n",
        "print(\"Matriz de Confusão:\\n\", conf_matrix)\n",
        "\n",
        "# Visualizar a matriz de confusão\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negativo', 'Neutro', 'Positivo'], yticklabels=['Negativo', 'Neutro', 'Positivo'])\n",
        "plt.ylabel('Rótulo Verdadeiro')\n",
        "plt.xlabel('Rótulo Previsto')\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.show()\n",
        "\n",
        "# Calcular a curva ROC e AUC\n",
        "# Para multi-classe, precisamos calcular a curva ROC e AUC para cada classe\n",
        "y_true_one_hot = torch.nn.functional.one_hot(torch.tensor(train_labels), num_classes=3).numpy()  # Supondo que você tenha 3 classes\n",
        "\n",
        "# Obtenha as probabilidades de cada classe\n",
        "outputs = model(batch['input_ids'], attention_mask=batch['attention_mask']).logits\n",
        "probabilities = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
        "\n",
        "# Calcular a curva ROC e AUC\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "for i in range(3):  # Supondo que você tenha 3 classes\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_one_hot[:, i], probabilities[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plotar a curva ROC\n",
        "plt.figure()\n",
        "for i in range(3):\n",
        "    plt.plot(fpr[i], tpr[i], label='Curva ROC Classe {0} (AUC = {1:0.2f})'.format(i, roc_auc[i]))\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsos Positivos')\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
        "plt.title('Curva ROC para Multiclasse')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8dewWv-zjMte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Defina o número de épocas\n",
        "num_epochs = 3  # Defina o número de épocas que você deseja treinar\n",
        "\n",
        "# Inicialize listas para armazenar as métricas\n",
        "train_precisions = []\n",
        "val_precisions = []\n",
        "\n",
        "train_recalls = []\n",
        "val_recalls = []\n",
        "\n",
        "train_f1s = []\n",
        "val_f1s = []\n",
        "\n",
        "# Crie DataLoader para os datasets de treinamento e validação\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Ajuste o tamanho do lote conforme necessário\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=16)  # Ajuste o tamanho do lote conforme necessário\n",
        "\n",
        "# Loop de treinamento\n",
        "for epoch in range(num_epochs):\n",
        "    # Treinamento\n",
        "    model.train()\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()  # Zera os gradientes\n",
        "        outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "        loss = loss_fn(outputs.logits, batch['sentiment'])\n",
        "        loss.backward()  # Calcula os gradientes\n",
        "        optimizer.step()  # Atualiza os pesos\n",
        "\n",
        "        # Obter predições\n",
        "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        train_preds.extend(preds)\n",
        "        train_labels.extend(batch['sentiment'].cpu().numpy())\n",
        "\n",
        "    # Calcular precisão, recall e f1-score no conjunto de treino\n",
        "    train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(train_labels, train_preds, average='weighted')\n",
        "    train_precisions.append(train_precision)\n",
        "    train_recalls.append(train_recall)\n",
        "    train_f1s.append(train_f1)\n",
        "\n",
        "    # Avaliação\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    val_probabilities = []\n",
        "\n",
        "    with torch.no_grad():  # Desativa o cálculo de gradientes\n",
        "        for batch in eval_loader:\n",
        "            outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            probabilities = torch.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy()  # Probabilidades da classe positiva\n",
        "\n",
        "            val_preds.extend(preds)\n",
        "            val_probabilities.extend(probabilities)\n",
        "            val_labels.extend(batch['sentiment'].cpu().numpy())\n",
        "\n",
        "    # Calcular precisão, recall e f1-score no conjunto de validação\n",
        "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
        "    val_precisions.append(val_precision)\n",
        "    val_recalls.append(val_recall)\n",
        "    val_f1s.append(val_f1)\n",
        "\n",
        "  # Convertendo rótulos e probabilidades para NumPy\n",
        "    val_labels = np.array(val_labels)\n",
        "    val_probabilities = np.array(val_probabilities)\n",
        "\n",
        "  # Gerar e plotar a curva Precision-Recall para cada classe (one-vs-rest)\n",
        "    for class_idx in range(3):  # Supondo que você tem 3 classes (0, 1 e 2)\n",
        "        precision, recall, _ = precision_recall_curve(val_labels == class_idx, val_probabilities[:, class_idx])\n",
        "        average_precision = average_precision_score(val_labels == class_idx, val_probabilities[:, class_idx])\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(recall, precision, label=f'AP = {average_precision:.2f}')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title(f'Precision-Recall Curve - Epoch {epoch + 1} - Classe {class_idx}')\n",
        "        plt.legend(loc='best')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "INc7an-eJFql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotar as métricas\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "# Plot precisão\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_precisions, label='Precisão de Treinamento')\n",
        "plt.plot(val_precisions, label='Precisão de Validação')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Precisão')\n",
        "plt.legend()\n",
        "plt.title('Precisão ao Longo das Épocas')\n",
        "\n",
        "# Plot F1-score\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_f1s, label='F1-Score de Treinamento')\n",
        "plt.plot(val_f1s, label='F1-Score de Validação')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.legend()\n",
        "plt.title('F1-Score ao Longo das Épocas')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oyg87oWnCwNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o modelo treinado\n",
        "model.save_pretrained('caminho_para_salvar_o_modelo')\n",
        "tokenizer.save_pretrained('caminho_para_salvar_o_tokenizer')\n"
      ],
      "metadata": {
        "id": "cGV6CPvwJF_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "segunda op."
      ],
      "metadata": {
        "id": "1iINHKPOfvf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    val_loss_total = 0  # Inicializar o acumulador de perda\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "            loss = loss_fn(outputs.logits, batch['sentiment'])\n",
        "            val_loss_total += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            val_preds.extend(preds)\n",
        "            val_labels.extend(batch['sentiment'].cpu().numpy())\n",
        "\n",
        "    # Calcular métricas na validação\n",
        "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
        "    val_precisions.append(val_precision)\n",
        "    val_recalls.append(val_recall)\n",
        "    val_f1s.append(val_f1)\n",
        "\n",
        "    # Exibir resultados da validação\n",
        "    print(f\"Validation Loss: {val_loss_total/len(eval_loader):.4f}\")\n",
        "    print(f\"Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, Validation F1: {val_f1:.4f}\")"
      ],
      "metadata": {
        "id": "ecjyPX7bf23O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Após o loop de treinamento, plotar as métricas\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "# Precision\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_precisions, 'b', label='Train Precision')\n",
        "plt.plot(epochs, val_precisions, 'r', label='Validation Precision')\n",
        "plt.title('Precision Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Recall\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_recalls, 'b', label='Train Recall')\n",
        "plt.plot(epochs, val_recalls, 'r', label='Validation Recall')\n",
        "plt.title('Recall Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# F1-Score\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_f1s, 'b', label='Train F1-Score')\n",
        "plt.plot(epochs, val_f1s, 'r', label='Validation F1-Score')\n",
        "plt.title('F1-Score Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LaY9IIaggTC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboard\n"
      ],
      "metadata": {
        "id": "CY_DucrKgfeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Inicialize o TensorBoard writer\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# Loop de treinamento\n",
        "for epoch in range(num_epochs):\n",
        "    # Treinamento\n",
        "    model.train()\n",
        "    train_loss_total = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "        loss = loss_fn(outputs.logits, batch['sentiment'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_total += loss.item()\n",
        "\n",
        "    # Escrever no TensorBoard\n",
        "    writer.add_scalar('Loss/train', train_loss_total / len(train_loader), epoch)\n",
        "    writer.add_scalar('Precision/train', train_precision, epoch)\n",
        "    writer.add_scalar('Recall/train', train_recall, epoch)\n",
        "    writer.add_scalar('F1/train', train_f1, epoch)\n",
        "\n",
        "    # Avaliação\n",
        "    model.eval()\n",
        "    val_loss_total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "            val_loss_total += loss.item()\n",
        "\n",
        "    writer.add_scalar('Loss/val', val_loss_total / len(eval_loader), epoch)\n",
        "    writer.add_scalar('Precision/val', val_precision, epoch)\n",
        "    writer.add_scalar('Recall/val', val_recall, epoch)\n",
        "    writer.add_scalar('F1/val', val_f1, epoch)\n",
        "\n",
        "# Ao final do treinamento\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "WYrl023cggNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard --logdir=runs\n"
      ],
      "metadata": {
        "id": "EK_M26UAglWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuração do ambiente de treinamento, permitindo o controle sobre o processo de otimização, salvamento de modelos e avaliação, garantindo o monitoraramento do desempenho ao longo do tempo."
      ],
      "metadata": {
        "id": "VAdnyxPSTPUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Cálculo da matriz de confusão\n",
        "y_true = y_true_val  # Rótulos verdadeiros de validação\n",
        "y_pred = y_pred_val  # Rótulos previstos\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "             xticklabels=['Negativo', 'Neutro', 'Positivo'],\n",
        "             yticklabels=['Negativo', 'Neutro', 'Positivo'])\n",
        "plt.ylabel('Rótulos Verdadeiros')\n",
        "plt.xlabel('Rótulos Previsto')\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "imVD8LsKIVuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Avaliação\n",
        "model.eval()\n",
        "y_true_val = []\n",
        "y_pred_val = []\n",
        "\n",
        "with torch.no_grad():  # Desativa o cálculo de gradientes\n",
        "    for batch in eval_loader:\n",
        "        outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()  # Predições do modelo\n",
        "        y_pred_val.extend(preds)  # Adiciona predições na lista\n",
        "        y_true_val.extend(batch['sentiment'].cpu().numpy())  # Adiciona rótulos verdadeiros na lista\n",
        "\n",
        "# Cálculo da matriz de confusão\n",
        "y_true = y_true_val  # Rótulos verdadeiros de validação\n",
        "y_pred = y_pred_val  # Rótulos previstos\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])  # Matriz de confusão para as 3 classes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Chxx8kv1V9Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibe a matriz de confusão de forma visual\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negativo', 'Neutro', 'Positivo'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Matriz de Confusão - Validação')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "del7YrDBHPF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criar o Trainer\n",
        "\n",
        "configura e inicia o treinamento do modelo BERT para classificação de sentimentos usando a biblioteca transformers\n"
      ],
      "metadata": {
        "id": "daTW-BwKpw_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# métricas de desempenho do modelo no conjunto de dados de **avaliação**"
      ],
      "metadata": {
        "id": "rPZn8AY-5eMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AVALIAÇÃO"
      ],
      "metadata": {
        "id": "nkyoFe5hkZ_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(eval_losses, label='Validation Loss', color='orange')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cr7ycbcOk6Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico de Barras para a Distribuição de Sentimentos"
      ],
      "metadata": {
        "id": "7ZizTaaLGZAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supondo que a coluna 'label' contém 0 (Negativo), 1 (Positivo), e 2 (Neutro)\n",
        "sentiment_counts = df_PREFEITO['sentiment'].value_counts()\n",
        "\n",
        "# Configurações do gráfico\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')\n",
        "plt.title('Distribuição de Sentimentos')\n",
        "plt.xlabel('Sentimento')\n",
        "plt.ylabel('Quantidade de Textos')\n",
        "plt.xticks(ticks=[0, 1, 2], labels=['Negativo', 'Positivo', 'Neutro'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KYlBE0XZGYXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico de Linha para Evolução Temporal dos Sentimentos\n"
      ],
      "metadata": {
        "id": "cTpVetTVGeF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuvem de Palavras"
      ],
      "metadata": {
        "id": "1UXoK3eoGhct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar a distribuição dos rótulos\n",
        "print(df_PREFEITO['sentiment'].value_counts())\n"
      ],
      "metadata": {
        "id": "V_GDflJkaqjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar se há textos vazios ou nulos\n",
        "print(df_PREFEITO[df_PREFEITO['sentiment'] == 2]['cleaned_text'].isna().sum())  # Verifica textos nulos\n",
        "print(df_PREFEITO[df_PREFEITO['sentiment'] == 2]['cleaned_text'].str.strip().eq('').sum())  # Verifica textos vazios\n"
      ],
      "metadata": {
        "id": "5Qpzx785atKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriz de Confusão\n"
      ],
      "metadata": {
        "id": "vhnx3JbGGlOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Suponha que y_true são os rótulos verdadeiros e y_pred as previsões do modelo\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Configurações do gráfico\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negativo', 'Positivo', 'Neutro'], yticklabels=['Negativo', 'Positivo', 'Neutro'])\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.xlabel('Previsões')\n",
        "plt.ylabel('Valores Reais')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_qMnVLFwGkgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico de Perda e Acurácia do Modelo"
      ],
      "metadata": {
        "id": "BMs_MXgeGq3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo simples de visualização da perda (loss) ao longo das épocas\n",
        "epochs = [1, 2, 3]  # Substituir pelos valores reais\n",
        "training_loss = [0.5, 0.25, 0.15]  # Substituir pelos valores reais\n",
        "validation_loss = [0.55, 0.30, 0.20]  # Substituir pelos valores reais\n",
        "\n",
        "# Configurações do gráfico\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs, training_loss, label='Perda de Treinamento', marker='o')\n",
        "plt.plot(epochs, validation_loss, label='Perda de Validação', marker='o')\n",
        "plt.title('Evolução da Perda Durante o Treinamento')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Perda (Loss)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HaqZb52AGrQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Gráfico de Dispersão 3D\n",
        "\n",
        "Se você tem mais variáveis numéricas no seu dataset, o heatmap de correlação pode ser útil para identificar relações entre elas."
      ],
      "metadata": {
        "id": "J_tStSJ6I9Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Supondo que você já tem embeddings (vetores) para cada texto\n",
        "# Redução de dimensionalidade para 3D usando PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_embeddings = pca.fit_transform(text_embeddings)  # Substitua 'text_embeddings' pelos vetores reais\n",
        "\n",
        "# Configurações do gráfico\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(reduced_embeddings[:,0], reduced_embeddings[:,1], reduced_embeddings[:,2], c=df_concatenado['label'], cmap='viridis')\n",
        "\n",
        "# Adicionar legenda e rótulos\n",
        "legend = ax.legend(*scatter.legend_elements(), title=\"Sentimento\")\n",
        "ax.add_artist(legend)\n",
        "ax.set_title('Visualização 3D de Textos em Embeddings')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6OqdJ_KBI2TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Heatmap de Correlação Entre Variáveis\n",
        "Se você tem mais variáveis numéricas no seu dataset, o heatmap de correlação pode ser útil para identificar relações entre elas.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Me6PgWEIbgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mapeamento de rótulos\n",
        "label_mapping = {\"positivo\": 2, \"neutro\": 1, \"negativo\": 0}\n",
        "df_PREFEITO['sentiment'] = df_PREFEITO['sentiment'].map(label_mapping)\n",
        "\n",
        "# Criar a coluna text_length\n",
        "df_PREFEITO['text_length'] = df_PREFEITO['cleaned_text'].str.len()\n",
        "\n",
        "# Calcular a correlação entre o comprimento do texto e o sentimento\n",
        "corr = df_PREFEITO[['text_length', 'sentiment']].corr()\n",
        "\n",
        "# Configurações do gráfico\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Mapa de Calor das Correlações Entre Variáveis')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HKGwPPHYHEsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico de ROC (Receiver Operating Characteristic)\n",
        "Esse gráfico é utilizado para avaliar o desempenho do modelo de classificação. Ele mostra a relação entre a taxa de verdadeiros positivos e a taxa de falsos positivos."
      ],
      "metadata": {
        "id": "lgygoBe4HQuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Curve (Curva de Aprendizado)\n",
        "A curva de aprendizado ajuda a entender se o modelo está se ajustando bem ou sofrendo de overfitting ou underfitting."
      ],
      "metadata": {
        "id": "9lRUM0bdHnsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Supondo que 'cleaned_text' é a coluna que contém o texto\n",
        "df_PREFEITO['text_length'] = df_PREFEITO['cleaned_text'].str.len()\n",
        "\n",
        "# Calcular a correlação entre o comprimento do texto e o sentimento\n",
        "corr = df_PREFEITO[['text_length', 'sentiment']].corr()\n",
        "\n",
        "# Configurações do gráfico\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Mapa de Calor das Correlações Entre Variáveis')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OpmBVQOiHm6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico de Acurácia por Época\n",
        "Visualize a acurácia de validação ao longo das épocas para entender a evolução do desempenho do modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "xBh9MFWDHtSB"
      }
    }
  ]
}